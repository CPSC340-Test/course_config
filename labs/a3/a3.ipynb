{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 571 Lab Assignment 1\n",
    "\n",
    "__Due__: December 3, 2017 at 15:00\n",
    "\n",
    "## Instructions\n",
    "rubric={mechanics:3, writing:3}\n",
    "\n",
    "Follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/). \n",
    "\n",
    "In addition, you must use proper spelling and grammar.\n",
    "\n",
    "__Note__: \n",
    "\n",
    "- The non-optional rubric weights add to 100. \n",
    "- The optional rubric weights add to 5.\n",
    "- There are two optional questions worth no marks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: $k$-NN Fundamentals\n",
    "\n",
    "Here we will try classification of the famous handwritten digits data set. \n",
    "\n",
    "This data set exists in many forms; we will use the one bundled in `sklearn.datasets`. We will also use `sklearn` for classification.\n",
    "\n",
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out the documentation for the data by running `print(digits['DESCR'])`. We'll extract the features and labels for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits['data'] # this is the data with each 8x8 image \"flattened\" into a length-64 vector.\n",
    "Y = digits['target'] # these are the labels (0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plot of a random example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x117d4b240>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEICAYAAAByNDmmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117d356a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = random.randint(0, digits['images'].shape[0]-1) \n",
    "plt.imshow(digits['images'][idx], cmap='Greys_r')\n",
    "plt.title('This is a %d' % digits['target'][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(a) Fundamentals\n",
    "\n",
    "rubric={reasoning:5}\n",
    "\n",
    "1. How many features are there, and what are they?\n",
    "2. Which is closer to element 0 (`X[0]`) -- element 1 (`X[1]`) or element 2 (`X[2]`)? Report the two distances (Euclidean).\n",
    "3. Using the above information, if only elements 1 and 2 are used in a $k$-NN classifier with $k=1$, what would element 0 be classified as, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(b) Investigating error\n",
    "\n",
    "rubric={reasoning:5}\n",
    "\n",
    "You'll be using the scikit-learn implementation of the $k$-NN classifier. Documentation is available at http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html.\n",
    "\n",
    "Using `k=10`, fit a $k$-NN classifier using `X` and `Y`. Obtain predictions from `X`. \n",
    "\n",
    "1. What proportion of these predictions are incorrect? This is called the _error rate_.    \n",
    "2. Choose one case that was not predicted correctly. What was predicted, and what is the correct label? Plot the image, and comment on why you think the classifier made a mistake. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(c) One Nearest Neighbour error\n",
    "\n",
    "rubric={reasoning:5}\n",
    "\n",
    "Now fit the classifier using `k=1`, and again obtain predictions from `X`. \n",
    "\n",
    "1. What proportion of these predictions are incorrect? Briefly explain why this error rate is achieved (in one or two sentences; think about how the $k$-NN algorithm works).    \n",
    "2. With the above error rate in mind, if I give you a new handwritten digit (not in the data set), will the classifier _for sure_ predict the label correctly? Briefly explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Investigating $k$-NN Error\n",
    "\n",
    "This is a continuation of Exercise 1. Each part asks you to investigate some scenario.\n",
    "\n",
    "### 2(a) The influence of k\n",
    "\n",
    "rubric={code:5, reasoning:5, viz:5}\n",
    "\n",
    "Now, split the data into _training_ and _test_ sets. You can choose any reasonable fraction for training vs. testing (50% will do). \n",
    "\n",
    "__Note__: It's always a good idea to randomly shuffle the data before splitting, in case the data comes ordered in some way. (For example, if they are ordered by label, then your training set will be all the digits 0-4, and your test set all the digits 5-9, which would be bad... you might end up with 100% error!!) To shuffle your data, you can use [`numpy.random.shuffle`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.shuffle.html).\n",
    "\n",
    "For various values of $k$, fit (a.k.a. _train_) a classifier using the training data. Use that classifier to obtain an error rate when predicting on both the training and test sets, for each $k$. How do the training error and test error change with $k$? Make a plot to show the trends, and briefly comment on the insights that this plot yields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(b) Fundamental Tradeoff\n",
    "rubric={reasoning:5}\n",
    "\n",
    "Recall the two parts of the fundamental trade-off in machine learning:\n",
    "\n",
    "1. How small we can make the training error.\n",
    "2. How well the training error approximates the test error.\n",
    "\n",
    "In a $k$-nearest neighbour classifier, how does the parameter $k$ affect each of the two parts of the trade-off? What is one way that we could choose $k$ in practice?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(c) The influence of data partition\n",
    "\n",
    "rubric={reasoning:5}\n",
    "\n",
    "Now, choose your favourite value of $k$, but vary the proportion of data reserved for the training set, again obtaining training and test error rates for each partition of the data. Plot training and test error (on the same axes) vs. the proportion of training examples. Briefly comment on the insights that this plot yields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(d) Imbalanced training data\n",
    "\n",
    "rubric={reasoning:5, code:5}\n",
    "\n",
    "Now, take a subset of your training data so that you only retain the first 2% of the examples of digits 0-8, but keep 100% of the 9's. This is called an imbalanced training set. \n",
    "\n",
    "What is your training and test error, vs. $k$? How does it compare to your performance with all the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2(e) Imbalanced training data -- fair (Optional)\n",
    "\n",
    "rubric={reasoning:1}\n",
    "\n",
    "What we did in (d) above was an unfair comparison, because reducing the amount of training data will generally hurt performance. To compare, randomly remove examples from the original training set such that the number of examples is the same as in part (d) above. Now compare the training and test error to the results from (d). Briefly comment on the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(f) The influence of added noise (Optional; no marks)\n",
    "\n",
    "__This question is worth no marks.__\n",
    "\n",
    "Now, add noise to the training data: for each example, with probability 20% replace the training label with a label selected uniformly at random. Remake the plot vs. $k$. What do you observe? Is there a generalizable insight here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Re-scaling\n",
    "\n",
    "Often, features are measured on different scales. Consider the toy example `data/unequal_scales.csv`, which has two features (`x1` and `x2`) and four labels (`A` through `D`). Feel free to use R for this question (but it's up to you). \n",
    "\n",
    "__Note__: To annotate a `ggplot2` plot with a circle, you can add the function found in [Vincenzo's gist](https://gist.github.com/vincenzocoia/58b26f6778647be1803d0b7348e674ec) as a layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(a) Leaving the data as-is\n",
    "\n",
    "rubric={viz:5, reasoning:5}\n",
    "\n",
    "1. Make a scatterplot of `x1` and `x2`, and indicate the category using some other aesthetic (like colour and/or shape).\n",
    "2. Draw a circle, centered around the point `(x1,x2)=(1,0)`, containing the $k=10$ nearest points to `(1,0)`. \n",
    "3. What would the point `(1,0)` be categorized as, using these data with $k$-NN, $k=10$? Why? Comment on whether you think it's a good idea to categorize the point `(1,0)` using a circle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(b) Re-scaling\n",
    "\n",
    "rubric={code:5, reasoning:5}\n",
    "\n",
    "To prevent this problem from happening, we can re-scale the data so that each feature has unit variance. We can achieve this by _normalizing_ the data -- that is, subtracting by the sample mean, then dividing by the sample standard deviation (you can achieve this using the `scale` function in R).\n",
    "\n",
    "1. Normalize the two features. What does the point `(1,0)` become on this new scale?\n",
    "2. Make another scatterplot of the re-scaled features, with a circle centered around the re-scaled point containing the $k=10$ nearest neighbours. \n",
    "3. What would the re-scaled point be categorized as? Why? Comment on whether you think the re-scaling is appropriate, and why. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(c) Comparing errors\n",
    "\n",
    "rubric={viz:3, reasoning:3}\n",
    "\n",
    "Using all the data as training data, plot the training error as a function of $k$. Do this for both the original and scaled data. How do the errors compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 4: decision trees with binary features\n",
    "<!-- rubric={reasoning:5,code:4,mechanics:1} -->\n",
    "\n",
    "The file `newsgroups.pkl` is a Pickle file containing the following objects:\n",
    "* `groupnames`: The names of four newsgroups.\n",
    "* `wordlist`: A list of words that occur in posts to these newsgroups.\n",
    "*  `X`: A binary matrix as a scipy sparse matrix ([documentation](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.spmatrix.html#scipy.sparse.spmatrix); [more info](http://www.scipy-lectures.org/advanced/scipy_sparse/index.html)). Each row corresponds to a post, and each column corresponds to a word from the word list. A value of $1$ means that the word occured in the post. Note: if you want to convert a sparse matrix to a regular dense numpy array, do `X.toarray()`.\n",
    "* `y`: A vector with values $0$ through $3$, with the value corresponding to the newsgroup that the post came from (comp.*, rec.*, sci.*, talk.*).\n",
    "* `Xtest` and `ytest`: the word lists and newsgroup labels for additional newsgroup posts.\n",
    "\n",
    "\n",
    "The code below loads the data, fits a decision stump to the training data, and then evaluates the error of the model on the training data. Running the demo shows that decision stumps have a classification error of $0.60$, which is a bit better than just predicting the most common label (which obtains an error of $0.66$). The image below gives an interpretation of the decision stump that is learned:\n",
    "![](data/decisionStump.png)\n",
    "This is an interpretable but not very accurate model. \n",
    "\n",
    "Your tasks follow the code chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "If \"windows\" is absent,  predict \"talk.*\"\n",
      "If \"windows\" is present, predict \"comp.*\"\n",
      "Predicting...\n",
      "Training error with decision stump = 0.600296\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# A decision stump that splits on a single variable,\n",
    "# assuming that X is binary {0,1} and is a scipy sparse matrix\n",
    "# and y is categorical {0,1,2,3,...,C-1}\n",
    "class DecisionStump():\n",
    "    \n",
    "    def __init__(self, classnames, inputnames=None):\n",
    "        self.splitVariable = None\n",
    "        self.label1 = None\n",
    "        self.label0 = None\n",
    "        self.is_fit = False\n",
    "        self.classnames = classnames\n",
    "        self.inputnames = inputnames\n",
    "        self.C = len(classnames) # number of classes\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        N,D = X.shape\n",
    "\n",
    "        # Count the occurrences of each label\n",
    "        count = np.zeros(self.C)\n",
    "        for n in range(N):\n",
    "            count[y[n]] += 1\n",
    "        \n",
    "        # Get the error if you didn't split at all \n",
    "        # and just predicted the most popular class\n",
    "        maxLabel = np.argmax(count)\n",
    "        minError = np.sum(y != maxLabel) \n",
    "#         print('Error by just predicting class %d: %f' % (maxLabel, minError/len(y)))\n",
    "        \n",
    "        splitVariable = None\n",
    "        splitLabel0 = maxLabel\n",
    "        splitLabel1 = maxLabel\n",
    "\n",
    "        # Loop over features looking for the best split\n",
    "        for d in range(D):\n",
    "            # Count frequency of each class label when the feature is 1, and when it is 0\n",
    "            # We do things a bit weirdly because X is a sparse matrix\n",
    "            count1 = np.zeros(self.C)\n",
    "            for n in X[:,d].nonzero()[0]:\n",
    "                count1[y[n]] += 1\n",
    "            count0 = count - count1\n",
    "            \n",
    "            # Compute majority class (the class we would predict) in each case \n",
    "            maxLabel0 = np.argmax(count0)\n",
    "            maxLabel1 = np.argmax(count1)\n",
    "\n",
    "            # Construct a vector yhat containing the predictions\n",
    "            yhat = np.zeros(N) + maxLabel0\n",
    "            yhat[X[:,d].nonzero()[0]] = maxLabel1\n",
    "            # Compute number of classification errors\n",
    "            errors = np.sum(yhat != y)\n",
    "\n",
    "            # Compare to minimum error so far\n",
    "            if errors < minError:\n",
    "                # This is the lowest error, store this value\n",
    "                minError = errors\n",
    "                splitVariable = d\n",
    "                splitLabel1 = maxLabel1\n",
    "                splitLabel0 = maxLabel0\n",
    "\n",
    "        self.splitVariable = splitVariable\n",
    "        self.label1 = splitLabel1\n",
    "        self.label0 = splitLabel0\n",
    "        self.is_fit = True\n",
    "        \n",
    "    def predict(self, X):\n",
    "        M,D = X.shape\n",
    "\n",
    "        if self.splitVariable is None:\n",
    "            return self.label0 * np.ones(M)\n",
    "\n",
    "        yhat = np.zeros(M)\n",
    "        for m in range(M):\n",
    "            if X[m,self.splitVariable] == 1:\n",
    "                yhat[m] = self.label1\n",
    "            else:\n",
    "                yhat[m] = self.label0      \n",
    "        return yhat\n",
    "    \n",
    "    def show(self):\n",
    "        assert self.is_fit\n",
    "        if self.inputnames is None:\n",
    "            split_name = \"Variable %d\" % self.splitVariable\n",
    "        else:\n",
    "            split_name = self.inputnames[self.splitVariable]\n",
    "        if self.classnames is None:\n",
    "            class_name0 = \"Class %d\" % self.label0\n",
    "            class_name1 = \"Class %d\" % self.label1\n",
    "        else:\n",
    "            class_name0 = self.classnames[self.label0]\n",
    "            class_name1 = self.classnames[self.label1]\n",
    "\n",
    "        print('If \"%s\" is absent,  predict \"%s\"' % (split_name, class_name0))\n",
    "        print('If \"%s\" is present, predict \"%s\"' % (split_name, class_name1))\n",
    "\n",
    "        \n",
    "with open('data/newsgroups.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "classnames = data['groupnames']\n",
    "wordnames = data['wordlist']\n",
    "model = DecisionStump(classnames=classnames, inputnames=wordnames)\n",
    "print('Fitting...')\n",
    "model.fit(X, y)\n",
    "model.show()\n",
    "print('Predicting...')\n",
    "pred = model.predict(X)\n",
    "training_error = np.sum(pred != y)/len(y)\n",
    "print('Training error with decision stump = %f' % training_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(a) Implementing a decision tree\n",
    "\n",
    "rubric={reasoning:5}\n",
    "\n",
    "The documentation for the scikit-learn implementation of the decision tree classifier is at http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "1. Briefly discuss each of the parameters accepted by this class.\n",
    "2. Using sklearn's decision tree classifier, report the training and test accuracy when using a decision tree of depth $10$.\n",
    "\n",
    "__Note__: Above code is for demonstration, but you don't actually have to use it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(b) Decision Tree Picture (Optional)\n",
    "\n",
    "rubric={reasoning:1}\n",
    "\n",
    "By inspecting the decision tree, draw a picture (similar to the one above) showing the learned decision tree when the maximum depth is $2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(c) Entropy Criterion (Optional)\n",
    "\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Modify your code by creating an option to use information gain (entropy reduction) as a split criterion, rather than minimizing error. Compare the training and testing error in each case, with a tree of depth 10. Briefly discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(d) Continuous Features (Optional; no marks)\n",
    "\n",
    "__This is worth no marks.__\n",
    "\n",
    "Explain how you would modify the `DecisionStump` code to handle continuous features rather than only binary features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Contemplating decision trees\n",
    "\n",
    "rubric={reasoning:5}\n",
    "\n",
    "Recall the two parts of the fundamental bias-variance trade-off in machine learning:\n",
    "\n",
    "1. How small we can make the training error.\n",
    "2. How well the training error approximates the test error.\n",
    "\n",
    "In a decision tree classifier, how do the different parameters affect each of the two parts of the trade-off, if at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Training error vs. test error, overfitting\n",
    "rubric={reasoning:5,viz:3}\n",
    "\n",
    "Expanding on the above by focusing in on the tree depth parameter, make a plot with the depth of the decision tree on the $x$-axis (varying it from $1$ through $15$) and the error on the training data $\\{X,y\\}$ on the $y$-axis. Then, make the analogous plot with test error instead of training error. Briefly explain your results. \n",
    "\n",
    "You can use sklearn's decision tree classifier for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Decision tree regression (Optional) \n",
    "\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Decision trees can also be used for regression; see [here](http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html) and [here](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html). Read up on this a bit and briefly explain how this works. Compare Decision Tree Regression to some of the methods you are learning in DSCI 561."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
